{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taller: Construcción e implementación de modelos Bagging, Random Forest y XGBoost\n",
    "\n",
    "En este taller podrán poner en práctica sus conocimientos sobre la construcción e implementación de modelos de Bagging, Random Forest y XGBoost. El taller está constituido por 8 puntos, en los cuales deberan seguir las intrucciones de cada numeral para su desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos predicción precio de automóviles\n",
    "\n",
    "En este taller se usará el conjunto de datos de Car Listings de Kaggle donde cada observación representa el precio de un automóvil teniendo en cuenta distintas variables como año, marca, modelo, entre otras. El objetivo es predecir el precio del automóvil. Para más detalles puede visitar el siguiente enlace: [datos](https://www.kaggle.com/jpayne/852k-used-car-listings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de la información de archivo .csv\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/datasets/dataTrain_carListings.zip')\n",
    "\n",
    "# Preprocesamiento de datos para el taller\n",
    "data = data.loc[data['Model'].str.contains('Camry')].drop(['Make', 'State'], axis=1)\n",
    "data = data.join(pd.get_dummies(data['Model'], prefix='M'))\n",
    "data = data.drop(['Model'], axis=1)\n",
    "\n",
    "# Visualización dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X) y variable de interés (y)\n",
    "y = data['Price']\n",
    "X = data.drop(['Price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de datos en set de entrenamiento y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 1 - Árbol de decisión manual\n",
    "\n",
    "En la celda 1 creen un árbol de decisión **manualmente**  que considere los set de entrenamiento y test definidos anteriormente y presenten el RMSE y MAE del modelo en el set de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 1\n",
    "# Definición de parámetros y criterios de parada\n",
    "max_depth = 6\n",
    "num_pct = 10\n",
    "min_gain=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la función que calcula el MSE\n",
    "def mse(y):\n",
    "    if y.shape[0] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean((y - y.mean())**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_reduction(X_col, y, split):\n",
    "    \n",
    "    filter_l = X_col < split\n",
    "    y_l = y.loc[filter_l]\n",
    "    y_r = y.loc[~filter_l]\n",
    "    \n",
    "    n_l = y_l.shape[0]\n",
    "    n_r = y_r.shape[0]\n",
    "    \n",
    "    mse_y = mse(y)\n",
    "    mse_l = mse(y_l)\n",
    "    mse_r = mse(y_r)\n",
    "    \n",
    "    mse_reduction_ = mse_y - (n_l / (n_l + n_r) * mse_l + n_r / (n_l + n_r) * mse_r)\n",
    "      \n",
    "    return mse_reduction_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la función best_split para calcular cuál es la mejor variable y punto de corte para hacer la bifurcación del árbol\n",
    "def best_split(X, y, num_pct=10):\n",
    "    \n",
    "    features = range(X.shape[1])\n",
    "    \n",
    "    best_split = [0, 0, 0]  # j, split, reduction\n",
    "    \n",
    "    # Para todas las variables \n",
    "    for j in features:\n",
    "        \n",
    "        splits = np.percentile(X.iloc[:, j], np.arange(0, 100, 100.0 / (num_pct+1)).tolist())\n",
    "        splits = np.unique(splits)[1:]\n",
    "        \n",
    "        # Para cada partición\n",
    "        for split in splits:\n",
    "            reduction = mse_reduction(X.iloc[:, j], y, split)\n",
    "                        \n",
    "            if reduction > best_split[2]:\n",
    "                best_split = [j, split, reduction]\n",
    "    \n",
    "    return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la función tree_grow para hacer un crecimiento recursivo del árbol\n",
    "def tree_grow(X, y, level=0, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct):\n",
    "    \n",
    "    # Si solo es una observación\n",
    "    if X.shape[0] == 1:\n",
    "        tree = dict(y_pred=y.iloc[:1].values[0], level=level, split=-1, n_samples=1, gain=0)\n",
    "        return tree\n",
    "    \n",
    "    # Calcular la mejor división\n",
    "    j, split, gain = best_split(X, y, num_pct)\n",
    "    \n",
    "    # Guardar el árbol y estimar la predicción\n",
    "    y_pred = y.mean()\n",
    "    \n",
    "    tree = dict(y_pred=y_pred, level=level, split=-1, n_samples=X.shape[0], gain=gain)\n",
    "    # Revisar el criterio de parada \n",
    "    if gain < min_gain:\n",
    "        return tree\n",
    "    if max_depth is not None:\n",
    "        if level >= max_depth:\n",
    "            return tree   \n",
    "    \n",
    "    # Continuar creando la partición\n",
    "    filter_l = X.iloc[:, j] < split\n",
    "    X_l, y_l = X.loc[filter_l], y.loc[filter_l]\n",
    "    X_r, y_r = X.loc[~filter_l], y.loc[~filter_l]\n",
    "    tree['split'] = [j, split]\n",
    "\n",
    "    # Siguiente iteración para cada partición\n",
    "    \n",
    "    tree['sl'] = tree_grow(X_l, y_l, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct)\n",
    "    tree['sr'] = tree_grow(X_r, y_r, level + 1, min_gain=min_gain, max_depth=max_depth, num_pct=num_pct)\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la función tree_predict para hacer predicciones según las variables 'X' y el árbol 'tree'\n",
    "\n",
    "def tree_predict(X, tree):\n",
    "    \n",
    "    predicted = np.ones(X.shape[0])\n",
    "\n",
    "    # Revisar si es el nodo final\n",
    "    if tree['split'] == -1:\n",
    "        predicted = tree['y_pred']\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        j, split = tree['split']\n",
    "        filter_l = (X.iloc[:, j] < split)\n",
    "        X_l = X.loc[filter_l]\n",
    "        X_r = X.loc[~filter_l]\n",
    "\n",
    "        if X_l.shape[0] == 0:  # Si el nodo izquierdo está vacio solo continua con el derecho \n",
    "            predicted[~filter_l] = tree_predict(X_r, tree['sr'])\n",
    "        elif X_r.shape[0] == 0:  #  Si el nodo derecho está vacio solo continua con el izquierdo\n",
    "            predicted[filter_l] = tree_predict(X_l, tree['sl'])\n",
    "        else:\n",
    "            predicted[filter_l] = tree_predict(X_l, tree['sl'])\n",
    "            predicted[~filter_l] = tree_predict(X_r, tree['sr'])\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=tree_grow(X_train, y_train, level=0, min_gain=0.001, max_depth=6, num_pct=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=tree_predict(X_test, tree)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "maetree = mean_absolute_error(y_test, y_pred)\n",
    "rmsetree = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"MAE Tree: \", maetree)\n",
    "print(\"RMSE: \", rmsetree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 2 - Bagging manual\n",
    "\n",
    "En la celda 2 creen un modelo bagging **manualmente** con 10 árboles de regresión y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 2\n",
    "# Creación de 10 muestras de bootstrap \n",
    "np.random.seed(123)\n",
    "\n",
    "n_samples = X_train.shape[0]\n",
    "n_B = 10\n",
    "\n",
    "samples = [np.random.choice(a=n_samples, size=n_samples, replace=True) for _ in range(1, n_B +1 )]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcción un árbol de decisión para cada muestra boostrap\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Definición del modelo usando DecisionTreeRegressor de sklearn\n",
    "treereg = DecisionTreeRegressor(max_depth=None, random_state=123)\n",
    "\n",
    "# DataFrame para guardar las predicciones de cada árbol\n",
    "y_pred = pd.DataFrame(index=X_test.index, columns=[list(range(n_B))])\n",
    "\n",
    "# Entrenamiento de un árbol sobre cada muestra boostrap y predicción sobre los datos de test\n",
    "for i in range(n_B):\n",
    "    X_b = X_train.iloc[samples[i],]\n",
    "    y_b = y_train.iloc[samples[i],]\n",
    "    treereg=DecisionTreeRegressor().fit(X_b, y_b)\n",
    "    y_pred.iloc[:,i] = treereg.predict(X_test)\n",
    "    \n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones promedio para cada obserbación del set de test\n",
    "y_pred=y_pred.mean(axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error al promediar las predicciones de todos los árboles\n",
    "maebagm = mean_absolute_error(y_test, y_pred)\n",
    "rmsebagm = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"MAE BagM: \", maebagm)\n",
    "print(\"RMSE BagM: \", rmsebagm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 3 - Bagging con librería\n",
    "\n",
    "En la celda 3, con la librería sklearn, entrenen un modelo bagging con 10 árboles de regresión y el parámetro `max_features` igual a `log(n_features)` y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 3\n",
    "# Uso de BaggingRegressor de la libreria (sklearn) donde se usa el modelo DecisionTreeRegressor como estimador\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "import math\n",
    "bagreg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=10, max_features=int(math.log(X.shape[1])),\n",
    "                          bootstrap=True, oob_score=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenemiento del modelo con set de entrenamiento y predicción en el set de test\n",
    "bagreg.fit(X_train, y_train)\n",
    "y_pred = bagreg.predict(X_test)\n",
    "maebagl = mean_absolute_error(y_test, y_pred)\n",
    "rmsebagl = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"MAE BagL: \", maebagl)\n",
    "print(\"RMSE BagL: \", rmsebagl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 4 - Random forest con librería\n",
    "\n",
    "En la celda 4, usando la librería sklearn entrenen un modelo de Randon Forest para regresión  y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 4\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg=RandomForestRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred=reg.predict(X_test)\n",
    "\n",
    "maerf = mean_absolute_error(y_test, y_pred)\n",
    "rmserf = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"MAE RF: \", maerf)\n",
    "print(\"RMSE RF: \", rmserf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 5 - Calibración de parámetros Random forest\n",
    "\n",
    "En la celda 5, calibren los parámetros max_depth, max_features y n_estimators del modelo de Randon Forest para regresión, comenten sobre el desempeño del modelo y describan cómo cada parámetro afecta el desempeño del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 5\n",
    "# Creación de lista de valores para iterar sobre diferentes valores de max_depth\n",
    "depth_range = range(1, 20 , 2)\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de max_depth\n",
    "rmse = []\n",
    "\n",
    "# Uso de un 5-fold cross-validation para cada valor de max_depth\n",
    "for depth in depth_range:\n",
    "    clf = RandomForestRegressor(max_depth=depth, random_state=1, n_jobs=-1)\n",
    "    rmse.append(-cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica del desempeño del modelo vs la cantidad de max_depth\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(depth_range, rmse)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de lista de valores para iterar sobre diferentes valores de max_features\n",
    "feature_range = range(1, X.shape[1]+1)\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de max_features\n",
    "rmse = []\n",
    "\n",
    "# Uso de un 5-fold cross-validation para cada valor de max_features\n",
    "for features in feature_range:\n",
    "    clf = RandomForestRegressor(max_features=features, random_state=1, n_jobs=-1)\n",
    "    rmse.append(-cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica del desempeño del modelo vs la cantidad de max_features\n",
    "plt.plot(feature_range, rmse)\n",
    "plt.xlabel('max_features')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de lista de valores para iterar sobre diferentes valores de n_estimators\n",
    "estimator_range = range(10, 210, 10)\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de n_estimators\n",
    "rmse = []\n",
    "\n",
    "# Uso de un 5-fold cross-validation para cada valor de n_estimators\n",
    "for estimator in estimator_range:\n",
    "    clf = RandomForestRegressor(n_estimators=estimator, random_state=1, n_jobs=-1)\n",
    "    rmse.append(-cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica del desempeño del modelo vs la cantidad de n_estimators\n",
    "plt.plot(estimator_range, rmse)\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg=RandomForestRegressor(max_depth=7,max_features=9,n_estimators=50)\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred=reg.predict(X_test)\n",
    "\n",
    "maerfc = mean_absolute_error(y_test, y_pred)\n",
    "rmserfc = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"MAE RFC: \", maerfc)\n",
    "print(\"RMSE RFC: \", rmserfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "reg=RandomForestRegressor()\n",
    "\n",
    "# Parámetros\n",
    "param_grid = {'max_depth': [4, 8],\n",
    "              'max_features': [3, 6],\n",
    "              'n_estimators': [300, 600]}\n",
    "\n",
    "# Búsqueda mejor modelo\n",
    "grid_search = GridSearchCV(estimator=reg, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "# Mejores parámetros\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "# Estimar el modelo\n",
    "best_regressor = grid_search.best_estimator_\n",
    "\n",
    "# Predicción\n",
    "y_pred = best_regressor.predict(X_test)\n",
    "\n",
    "maerfc = mean_absolute_error(y_test, y_pred)\n",
    "rmserfc = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"MAE RFC: \", maerfc)\n",
    "print(\"RMSE RFC: \", rmserfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 6 - XGBoost con librería\n",
    "\n",
    "En la celda 6 implementen un modelo XGBoost de regresión con la librería sklearn y comenten sobre el desempeño del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 6\n",
    "from xgboost import XGBRegressor\n",
    "reg = XGBRegressor()\n",
    "# Entrenamiento (fit) y desempeño del modelo XGBClassifier\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "maexgb = mean_absolute_error(y_test, y_pred)\n",
    "rmsexgb = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"MAE XGB: \", maexgb)\n",
    "print(\"RMSE XGB: \", rmsexgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 7 - Calibración de parámetros XGBoost\n",
    "\n",
    "En la celda 7 calibren los parámetros learning rate, gamma y colsample_bytree del modelo XGBoost para regresión, comenten sobre el desempeño del modelo y describan cómo cada parámetro afecta el desempeño del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 7\n",
    "# Creación de lista de valores para iterar sobre diferentes valores de max_features\n",
    "rate_range = np.arange (0.01, 0.2, 0.01)\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de max_features\n",
    "rmse = []\n",
    "\n",
    "# Uso de un 5-fold cross-validation para cada valor de max_features\n",
    "for rate in rate_range:\n",
    "    clf = XGBRegressor(learning_rate=rate, random_state=1, n_jobs=-1)\n",
    "    rmse.append(-cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica del desempeño del modelo vs la cantidad de n_estimators\n",
    "plt.plot(rate_range, rmse)\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de lista de valores para iterar sobre diferentes valores de max_features\n",
    "gamma_range = np.arange (0, 30000, 1000)\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de max_features\n",
    "rmse = []\n",
    "\n",
    "# Uso de un 5-fold cross-validation para cada valor de max_features\n",
    "for gamma in gamma_range:\n",
    "    clf = XGBRegressor(gamma=gamma, random_state=1, n_jobs=-1)\n",
    "    rmse.append(-cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica del desempeño del modelo vs la cantidad de n_estimators\n",
    "plt.plot(gamma_range, rmse)\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de lista de valores para iterar sobre diferentes valores de max_features\n",
    "col_range = np.arange (0, 1, 0.01)\n",
    "\n",
    "# Definición de lista para almacenar la exactitud (accuracy) promedio para cada valor de max_features\n",
    "rmse = []\n",
    "\n",
    "# Uso de un 5-fold cross-validation para cada valor de max_features\n",
    "for col in col_range:\n",
    "    clf = XGBRegressor(colsample_bytree=col, random_state=1, n_jobs=-1)\n",
    "    rmse.append(-cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_root_mean_squared_error').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica del desempeño del modelo vs la cantidad de n_estimators\n",
    "plt.plot(col_range, rmse)\n",
    "plt.xlabel('colsample_bytree')\n",
    "plt.ylabel('RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "reg = XGBRegressor(learning_rate=0.025,gamma=30000,colsample_bytree=0.2)\n",
    "# Entrenamiento (fit) y desempeño del modelo XGBClassifier\n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "maexgbc = mean_absolute_error(y_test, y_pred)\n",
    "rmsexgbc = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "print(\"MAE XGBC: \", maexgbc)\n",
    "print(\"RMSE XGBC: \", rmsexgbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punto 8 - Comparación y análisis de resultados\n",
    "En la celda 8 comparen los resultados obtenidos de los diferentes modelos (random forest y XGBoost) y comenten las ventajas del mejor modelo y las desventajas del modelo con el menor desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda 8\n",
    "print(\"MAE RF: \", maerf)\n",
    "print(\"RMSE RF: \", rmserf)\n",
    "print(\"MAE RFC: \", maerfc)\n",
    "print(\"RMSE RFC: \", rmserfc)\n",
    "print(\"MAE XGB: \", maexgb)\n",
    "print(\"RMSE XGB: \", rmsexgb)\n",
    "print(\"MAE XGBC: \", maexgbc)\n",
    "print(\"RMSE XGBC: \", rmsexgbc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
